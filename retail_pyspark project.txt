from pyspark.sql.types import StructType, StringType, IntegerType, DoubleType, LongType

schema10 = StructType().add("txn_id",StringType(),True).add("cust_id",StringType(),True).add("age",StringType(),True).add("zipcode",StringType(),True).add("category",StringType(),True).add("product",StringType(),True).add("qty",IntegerType(),True).add("cost",IntegerType(),True).add("sale",IntegerType(),True)

retailDF = spark.read.format("csv").option("sep", ";").option("header","False").schema(schema10).load("hdfs://nameservice1/user/bigcdac432585/retail")

retailDF.registerTempTable("retail")


retailDF.count()   		//for counting rows in DataFrame

==================================================================
1)	Count of unique customers and total sales for each age group and for a given month = Jan

Hint : where month(txn_dt) = 1

A	5000		600000
B	4500		540000


solution-1 ==> ///"sum can't work without group by"
---------------
sol_1=spark.sql("select count(distinct(txn_id)), age, sum(sale) as total from retail where month(txn_id) =1 group by age order by total")
sol_1.show()    		//date should be in string to show in where clause 

==================================================================

2) Count of unique customers and total sales for one age group(A) for all products - [ sort data on totalsales desc- find top 10]

	ProdA	count of unique cust	total sales
	ProdB	‘’					‘’

Hint : where trim(age)  = 'A'

solution-2 ==>
------------
sol_2 = spark.sql("select count(distinct(txn_id)), product, sum(sale) from retail where trim(age) = 'A' group by product limit 10")
sol_2.show()     //to show the query 
==================================================================

3) Area wise sales   

solution-3 ==>
------------
sol_3 = spark.sql("select zipcode, sum(sale) from retail group by zipcode")
sol_3.show() 
==================================================================

4) find top 10 viable products (prod which give highest profit)

solution-4 ==>
------------
sol_4 = spark.sql("select product, sum((sale-cost)) as total from retail group by product order by total desc limit 10")
sol_4.show() 
==================================================================

5) find all loss making products – Display all the loss making products from highest loss to the least

solution-5 ==>
------------
sol_5 = spark.sql("select product ,sum((sale -cost)) as profit from retail group by product having profit <0 order by profit")
sol_5.show()



df_StockVol = spark.sql("select stock_id, sum(volume) from nyse group by stock_id")